# -*- coding: utf-8 -*-
"""Progetto - Data Augmentation per la sicurezza delle centrali elettriche.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rw0WlHIZQx0fW5fTgXQAF0L4_cSGUQfo

#Introduzione

CyberEye Solutions, un leader emergente nel settore della sicurezza cibernetica per infrastrutture critiche, si trova di fronte a una crescente sfida nella protezione delle centrali elettriche contro minacce informatiche avanzate. Attualmente, il sistema di sorveglianza delle centrali utilizza tecnologie di riconoscimento di immagini per identificare e reagire tempestivamente a situazioni potenzialmente pericolose. Tuttavia, la capacitÃ  di riconoscere con precisione e tempestivitÃ  oggetti e comportamenti critici nelle immagini Ã¨ limitata dai dataset di addestramento attualmente disponibili, che non rappresentano appieno la variabilitÃ  e la complessitÃ  delle situazioni reali.

Attualmente, i modelli esistenti basati su dataset limitati non riescono a rilevare con la necessaria precisione anomalie o minacce potenziali nelle immagini, compromettendo la capacitÃ  di risposta e mitigazione dell'azienda di fronte a situazioni di emergenza. Migliorare la capacitÃ  di identificare tempestivamente oggetti e comportamenti critici nelle immagini Ã¨ fondamentale per garantire la continuitÃ  operativa e la sicurezza delle infrastrutture critiche gestite da CyberEye Solutions.

**Benefici della Soluzione**

1. **Miglioramento della Sicurezza delle Infrastrutture Critiche**: Espandere il dataset utilizzando tecniche avanzate di Data Augmentation consentirÃ  di migliorare l'accuratezza del sistema di riconoscimento di immagini. Un modello piÃ¹ preciso e affidabile sarÃ  in grado di rilevare con maggiore tempestivitÃ  e precisione comportamenti sospetti o minacce potenziali nelle immagini delle centrali elettriche, migliorando cosÃ¬ la sicurezza delle infrastrutture critiche e riducendo il rischio di incidenti o sabotaggi.
2. **Efficienza Operativa e Riduzione del Tempo di Risposta**: Automatizzando il processo di generazione di nuovi dati attraverso la creazione di immagini e testi variati, CyberEye Solutions ottimizzerÃ  l'efficienza operativa. Questo permetterÃ  all'azienda di concentrare le risorse umane su attivitÃ  di analisi e mitigazione delle minacce, riducendo il tempo di risposta agli eventi critici e migliorando la capacitÃ  di gestione delle emergenze.
3. **Innovazione Tecnologica nel Settore della Sicurezza**: Utilizzando tecniche avanzate di deep learning e generazione di dati, CyberEye Solutions promuoverÃ  l'innovazione nel campo della sicurezza cibernetica per infrastrutture critiche. L'implementazione di modelli di riconoscimento di immagini piÃ¹ sofisticati non solo migliorerÃ  la sicurezza delle centrali elettriche, ma dimostrerÃ  anche l'impegno dell'azienda nell'adozione di tecnologie all'avanguardia per affrontare le sfide emergenti nel settore della sicurezza cibernetica.

**Dettagli del Progetto**

- **Acquisizione del Dataset**: Utilizzare il dataset OxfordIIITPet da PyTorch come base per il progetto di miglioramento del sistema di riconoscimento di immagini per infrastrutture critiche.
- **Image Captioning e Generazione di Dati**: Applicare lâ€™image captioning per creare descrizioni iniziali delle immagini. Successivamente, utilizzare un modello generativo di testo per produrre varianti o descrizioni analoghe. Infine, impiegare un modello generativo di immagini per creare nuove immagini a partire dalle caption originali o dai testi generati, arricchendo cosÃ¬ il dataset con dati sintetici.
- **Addestramento del Modello**: Addestrare un modello di riconoscimento di immagini utilizzando il dataset esteso, valutando la qualitÃ  dei dati prodotti e confrontando le performance del modello su dataset ridotto e dataset incrementato.
- **Valutazione delle Performance**: Misurare l'accuracy, precision, recall e altre metriche di performance per confrontare il modello addestrato su entrambi i dataset. Commentare le differenze nelle performance e l'efficacia delle tecniche di Data Augmentation nel migliorare l'accuratezza del modello in contesti reali di sicurezza delle infrastrutture critiche.

**Conclusioni**

CyberEye Solutions si impegna a rafforzare la sicurezza delle infrastrutture critiche attraverso l'implementazione di soluzioni avanzate di riconoscimento di immagini. Utilizzando approcci innovativi e tecnologie all'avanguardia, l'azienda mira non solo a migliorare l'efficacia dei suoi sistemi di sicurezza cibernetica, ma anche a definire nuovi standard nel settore per la protezione delle infrastrutture critiche contro le minacce informatiche sempre piÃ¹ sofisticate.

#Progetto

##Installazioni e Import Librerie
"""

!pip install transformers pillow diffusers torch torchvision accelerate timm tensorboard

# Librerie standard
import time
import os
import copy
import collections

# Librerie scientifiche
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import random

# Librerie PyTorch
import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split, ConcatDataset
from torch.utils.tensorboard import SummaryWriter
from torchvision import datasets, transforms
from torchvision.utils import make_grid

# Librerie di Deep Learning e Vision
import timm
from transformers import BlipProcessor, BlipForConditionalGeneration, AutoModelForSeq2SeqLM, AutoTokenizer
from diffusers import StableDiffusionPipeline
from IPython.display import display
from collections import Counter

# Metriche e valutazione
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report

random.seed(42)
np.random.seed(42)
torch.manual_seed(42)

# Aumento timeout download HuggingFace Hub (default 10s â†’ 60s)
os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "60"

# Definizione dellâ€™ID del modello Stable Diffusion e scelta del device
model_id = "CompVis/stable-diffusion-v1-4"
# Verifica disponobilitÃ  GPU CUDA, altrimenti fallback su CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Utilizzo del dispositivo: {device}")

# Caricamento della pipeline di Stable Diffusion in formato FP16 per ridurre lâ€™uso di memoria
# cache_dir: directory locale per salvare i pesi ed evitare download ripetuti
pipe = StableDiffusionPipeline.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    revision="fp16",
    cache_dir="./hf_cache"
)
# Spostamento pipeline sul device scelto (GPU o CPU)
pipe = pipe.to(device)

# Impostazioni di logging
log_dir = "./logs"
writer = SummaryWriter(log_dir)

# Caricamento pipeline di image captioning (BLIP)
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model.to(device)
blip_model.eval()

# Caricamento modello di testo (Flan-T5) per varianti
flan_model_name = "google/flan-t5-small"
tokenizer = AutoTokenizer.from_pretrained(flan_model_name)
flan_model = AutoModelForSeq2SeqLM.from_pretrained(flan_model_name)
flan_model.to(device)
flan_model.eval()

"""##Acquisizione e Preparazione del Dataset

###Funzioni
"""

# -----------------------------------
# Funzione: split_dataset
#   Suddivide un dataset in training e validation.
# Params:
#   - dataset: PyTorch Dataset da suddividere
#   - split_ratio (float): frazione da usare per il training (default 0.8)
# Returns:
#   - train_set, val_set: due sottodataset con le proporzioni richieste
# -----------------------------------
def split_dataset(dataset, split_ratio: float = 0.8):
    """
    Suddivide il dataset in due parti, train e validation, mantenendo l'ordine casuale.
    """
    train_size = int(len(dataset) * split_ratio)
    val_size = len(dataset) - train_size
    return random_split(dataset, [train_size, val_size])

# -----------------------------------
# Funzione: imshow
#   Visualizza un batch di immagini denormalizzate.
# Params:
#   - img: Tensor di shape (C, H, W) normalizzato con mean/std di ImageNet
#   - title (str, opzionale): titolo da mostrare sopra l'immagine
# -----------------------------------
def imshow(img, title: str = None):
    """
    Denormalizza e mostra un'immagine o una griglia di immagini.
    Il tensor in ingresso deve essere nei formati abituali di PyTorch (C,H,W).
    """
    npimg = img.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std  = np.array([0.229, 0.224, 0.225])
    npimg = std * npimg + mean
    npimg = np.clip(npimg, 0, 1)

    plt.figure(figsize=(8, 8))
    plt.imshow(npimg)
    if title:
        plt.title(title)
    plt.axis('off')
    plt.show()

# -----------------------------------
# Funzione: generate_caption
#   Genera una didascalia per un'immagine PIL con BLIP.
# Params:
#   - image: PIL.Image.Image da descrivere
# Returns:
#   - caption (str)
# -----------------------------------
def generate_caption(image: Image.Image) -> str:
    """
    Crea una caption testuale per l'immagine utilizzando BLIP.
    Si assume che `processor` e `blip_model` siano giÃ  caricati e su `device`.
    """
    inputs = processor(image, return_tensors="pt").to(device)
    with torch.no_grad():
        output_ids = blip_model.generate(**inputs)
    caption = processor.decode(output_ids[0], skip_special_tokens=True)
    return caption


# -----------------------------------
# Funzione: generate_caption_variants
#   Espande una caption originale in piÃ¹ varianti semantiche.
# Params:
#   - original_caption (str): didascalia di partenza
#   - num_variants (int): numero di varianti da generare (default 3)
# Returns:
#   - List[str]: didascalie varianti
# -----------------------------------
def generate_caption_variants(original_caption: str, num_variants: int = 3) -> list:
    """
    Genera `num_variants` riformulazioni della caption originale
    utilizzando il modello Flan-T5. Utile per aumentare la diversitÃ  del testo.
    """
    prompt = f"Generate {num_variants} different captions with similar meaning: {original_caption}"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = flan_model.generate(
            **inputs,
            max_length=50,
            num_return_sequences=num_variants,
            do_sample=True,
            temperature=0.9,
            top_k=50
        )
    variants = [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]
    return variants

# -----------------------------------
# Funzione: generate_image_from_caption
#   Crea un'immagine sintetica da una caption con Stable Diffusion.
# Params:
#   - caption (str): testo di input per la pipeline
# Returns:
#   - PIL.Image.Image: immagine sintetica generata
# -----------------------------------
def generate_image_from_caption(caption: str) -> Image.Image:
    """
    Utilizza la pipeline `pipe` giÃ  caricata per generare un'immagine.
    Prova fino a 3 volte in caso di errori di rete o timeout.
    """
    for attempt in range(1, 4):
        try:
            result = pipe(caption, num_inference_steps=50, guidance_scale=7.5)
            return result.images[0]
        except Exception as e:
            print(f"[Tentativo {attempt}/3] Errore generazione immagine: {e}")
    raise RuntimeError("Fallito a generare immagine dopo 3 tentativi")

# -----------------------------------
# Funzione: augment_image
#   Integra tutti i passaggi generativi (captioning â†’ varianti â†’ sintesi).
# Params:
#   - image: PIL.Image.Image di partenza
#   - num_variants (int): numero di varianti/caption e immagini da creare
# Returns:
#   - List[PIL.Image.Image]: immagini sintetiche generate
# -----------------------------------
def augment_image(image: Image.Image, num_variants: int = 3) -> list:
    """
    Esegue pipeline completa di augmentazione generativa:
    1. Genera caption con BLIP
    2. Crea varianti della caption con Flan-T5
    3. Genera un'immagine per ciascuna variante via Stable Diffusion
    """
    orig_caption = generate_caption(image)
    print(f"Caption originale: {orig_caption}")

    caption_variants = generate_caption_variants(orig_caption, num_variants=num_variants)
    print(f"Varianti generate: {caption_variants}")

    generated_images = []
    for cap in caption_variants:
        img = generate_image_from_caption(cap)
        generated_images.append(img)

    return generated_images

# -----------------------------------
# Funzione: tensor_to_pil
#   Converte un tensore normalizzato in un'immagine PIL
# Input:
#   - tensor_img (torch.Tensor): shape (C, H, W), valori normalizzati secondo ImageNet
# Output:
#   - PIL.Image.Image: l'immagine denormalizzata
# -----------------------------------
def tensor_to_pil(tensor_img: torch.Tensor) -> Image.Image:
    """
    Trasforma un tensore (C,H,W) normalizzato in un'immagine PIL.
    Utilizza le medie e deviazioni standard di ImageNet per denormalizzare.
    """
    # Da torch.Tensor a NumPy HÃ—WÃ—C
    npimg = tensor_img.cpu().numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std  = np.array([0.229, 0.224, 0.225])
    # Denormalizzazione
    npimg = std * npimg + mean
    # Clippa tra 0 e 1
    npimg = np.clip(npimg, 0, 1)
    # Porta a 0â€“255 e converte in uint8
    npimg = (npimg * 255).astype('uint8')
    # Da array PIL
    return Image.fromarray(npimg)

"""###Workflow"""

# Definizione delle trasformazioni di preprocessing
#    - Resize a 224Ã—224
#    - Conversione in Tensor
#    - Normalizzazione con mean/std ImageNet
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# Download e caricamento del dataset Oxford-IIIT Pet
#   - split='trainval' restituisce insieme train+val
data_dir = './data'
train_dataset = datasets.OxfordIIITPet(
    root=data_dir,
    split='trainval',
    download=True,
    transform=transform
)

# Preparazione cartella per le immagini sintetiche
synthetic_data_dir = "./synthetic_data"
os.makedirs(synthetic_data_dir, exist_ok=True)

# Suddivisione in training e validation del dataset originale
#   - Usando split_dataset() definita a inizio script
train_set, val_set = split_dataset(train_dataset)
batch_size = 32
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_set,   batch_size=batch_size, shuffle=False)
print("Dataset originale caricato e suddiviso correttamente.")

# Visualizzazione di qualche batch iniziale, per verificare che il preprocessing sia corretto
dataiter = iter(train_loader)
images, labels = next(dataiter)
imshow(torchvision.utils.make_grid(images), title='Esempi dal Training Set')

# Generazione immagini sintetiche da un'immagine originale tramite:
# - Captioning (BLIP),
# - Variazioni di caption,
# - Generazione immagini (Stable Diffusion)
synthetic_sample_dir = os.path.join(synthetic_data_dir, "class_0")
if not os.path.exists(synthetic_sample_dir) or len(os.listdir(synthetic_sample_dir)) == 0:
    os.makedirs(synthetic_sample_dir, exist_ok=True)
    sample_count = 10   # numero di immagini originali da cui partire
    count = 0

    for img_tensor, label in train_dataset:
        if label == 0:  # selezione classe 0 per il demo
            # Denormalizzazione tensore e conversione in PIL
            npimg = img_tensor.numpy().transpose((1, 2, 0))
            mean = np.array([0.485, 0.456, 0.406])
            std  = np.array([0.229, 0.224, 0.225])
            npimg = std * npimg + mean
            npimg = np.clip(npimg, 0, 1)
            pil_img = Image.fromarray((npimg * 255).astype('uint8'))

            # Applicazione pipeline di augment_image:
            #     - captioning con BLIP
            #     - generazione di varianti di caption
            #     - sintesi di nuove immagini con Stable Diffusion
            synthetic_images = augment_image(pil_img, num_variants=3)

            # Salvoataggio su disco
            for i, simg in enumerate(synthetic_images):
                filename = os.path.join(synthetic_sample_dir, f"img_{count}_aug_{i}.png")
                simg.save(filename)

            count += 1
            if count >= sample_count:
                break

    print(f"Generate {count * 3} immagini sintetiche in {synthetic_sample_dir}")
else:
    print("Dataset sintetico giÃ  presente, uso le immagini esistenti.")

# Creazione del dataset combinato(originale + sintetico, se presente)
if len(os.listdir(synthetic_data_dir)) > 0:
    synthetic_dataset = datasets.ImageFolder(root=synthetic_data_dir, transform=transform)
    print("Dataset sintetico caricato.")
    combined_dataset = ConcatDataset([train_dataset, synthetic_dataset])
else:
    combined_dataset = train_dataset
    print("Nessun dato sintetico trovato; utilizzo del dataset originale.")

# Suddivisione train/val del combined_dataset (80/20)
combined_train_set, combined_val_set = split_dataset(combined_dataset, split_ratio=0.8)
combined_train_loader = DataLoader(combined_train_set, batch_size=32, shuffle=True)
combined_val_loader = DataLoader(combined_val_set,   batch_size=32, shuffle=False)

print(f"combined_train_loader definito con {len(combined_train_set)} esempi")
print(f"combined_val_loader definito con {len(combined_val_set)} esempi")

# Conteggio numero esempi per classe
labels = [label for _, label in combined_train_set]
counts = Counter(labels)

print("ðŸ“Š Distribuzione degli esempi per classe nel dataset combinato:")
for cls_idx, cnt in sorted(counts.items()):
    cls_name = train_dataset.classes[cls_idx]
    print(f"  â€¢ Classe {cls_idx:2d} ({cls_name}): {cnt} esempi")

# Preparazione del test set (20% di combined_dataset)
test_size  = int(0.2 * len(combined_dataset))
train_size = len(combined_dataset) - test_size
_, test_set = random_split(combined_dataset, [train_size, test_size])

# Caricamento del test set in un DataLoader
test_loader = DataLoader(test_set, batch_size=32, shuffle=False)

print(f"Test loader creato con {len(test_set)} esempi.")

# Visualizzazione batch â€œgrezzoâ€ (controllo casuale)
batch_imgs, _ = next(iter(combined_train_loader))
imshow(torchvision.utils.make_grid(batch_imgs), title='Batch casuale dal combined_train_loader')

# 3 esempi per le prime 4 classi (campionamento mirato)
num_per_class   = 3
classes_to_show = list(counts.keys())[:4]
fig, axes = plt.subplots(len(classes_to_show), num_per_class,
                         figsize=(num_per_class*3, len(classes_to_show)*3))
for row, cls_idx in enumerate(classes_to_show):
    imgs = [img for img, lbl in combined_train_set if lbl == cls_idx][:num_per_class]
    for col, img_tensor in enumerate(imgs):
        ax = axes[row, col]
        ax.imshow(tensor_to_pil(img_tensor))
        ax.set_title(train_dataset.classes[cls_idx], fontsize=8)
        ax.axis('off')

plt.suptitle("Esempi per alcune classi dal combined_train_loader", fontsize=14)
plt.tight_layout()
plt.show()

print("âœ… Pipeline di acquisizione e preparazione del dataset completata.")

"""##Image Captioning

###Workflow
"""

# Selezione immagine dal dataset
dataiter = iter(train_loader)
images, _ = next(dataiter)

# Conversione prima immagine del batch in PIL e generazione caption
pil_image = tensor_to_pil(images[0])
caption = generate_caption(pil_image)
variants = generate_caption_variants(caption)

# Generazione caption tramite modello BLIP
print("Caption generata:", caption)
print("Varianti generate:", variants)

# Visualizzazione immagine con caption
plt.figure(figsize=(8, 8))
plt.imshow(pil_image)
plt.title("Caption: " + caption)
plt.axis("off")
plt.show()

"""##Generazione di Nuove Caption (Testo)

###Workflow
"""

# Esempio di utilizzo con caption generata in precedenza
original_caption = "a security camera monitoring an industrial plant"
caption_variants = generate_caption_variants(original_caption)

print("Original Caption:", original_caption)
print("Generated Variants:", caption_variants)

"""##Generazione di Nuove Immagini

###Funzioni
"""

# -----------------------------------
# Funzione: generate_image_from_caption
#   Genera un'immagine sintetica a partire da una didascalia testuale
#   utilizzando la pipeline Stable Diffusion (precaricata).
# Tentativi:
#   - Fino a 3 tentativi automatici in caso di errore di rete o timeout.
# Params:
#   - caption (str): descrizione testuale da convertire in immagine
# Returns:
#   - PIL.Image.Image: immagine generata dalla caption
# Raises:
#   - RuntimeError: se tutti e 3 i tentativi falliscono
# -----------------------------------
def generate_image_from_caption(caption: str) -> Image.Image:
    """
    Genera unâ€™immagine sintetica a partire da una didascalia testuale.

    Utilizza la pipeline Stable Diffusion pre-caricata (pipe) spostata
    sul device corretto allâ€™avvio del notebook. In caso di errore di
    rete o timeout, ritenta fino a 3 volte prima di sollevare unâ€™eccezione.

    Args:
        caption (str): La descrizione testuale da cui generare lâ€™immagine.

    Returns:
        PIL.Image.Image: Lâ€™immagine generata dalla pipeline.

    Raises:
        RuntimeError: Se dopo 3 tentativi non Ã¨ stato possibile generare lâ€™immagine.
    """
    for attempt in range(1, 4):
        try:
            # Chiamata alla pipeline giÃ  inizializzata
            result = pipe(
                caption,
                num_inference_steps=50,
                guidance_scale=7.5
            )
            return result.images[0]
        except Exception as e:
            print(f"[Tentativo {attempt}/3] Errore durante la generazione dellâ€™immagine: {e}")
    # Se tutti i retry falliscono
    raise RuntimeError(f"Impossibile generare lâ€™immagine dalla caption dopo 3 tentativi: '{caption}'")

"""###Workflow"""

# Caricamento modello pre-addestrato
model_id = "CompVis/stable-diffusion-v1-4"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipe.to(device)

# Generazione immagine tramite caption varianti
new_image = generate_image_from_caption(caption_variants[0])

# Salvataggio e visualizzazione immagine
new_image.save("generated_image.png")
display(new_image)

"""##Addestramento

###Funzioni
"""

# --------------------------------------------------
# FUNZIONI DI AUGMENTATION: MixUp e CutMix
# --------------------------------------------------

# -----------------------------------
# Funzione: mixup_data
#   Applica MixUp a un batch di immagini e label.
#   Miscela due immagini casuali con un peso lambda da distribuzione Beta.
# Params:
#   - x (torch.Tensor): batch di immagini (B, C, H, W)
#   - y (torch.Tensor): batch di etichette (B,)
#   - alpha (float): parametro della Beta distribution
# Returns:
#   - mixed_x: batch di immagini miscelate
#   - y_a, y_b: etichette corrispondenti per la loss
#   - lam: coefficiente lambda usato nella miscela
# -----------------------------------
def mixup_data(x: torch.Tensor, y: torch.Tensor, alpha: float = 0.15):
    """
    Applica MixUp a un batch.

    Args:
        x: batch di immagini, shape (B, C, H, W)
        y: batch di label, shape (B,)
        alpha: parametro della Beta distribution per pesare le miscele

    Returns:
        mixed_x: immagini miscelate
        y_a, y_b: coppie di label per il calcolo della loss
        lam: coefficiente lambda usato per la miscelazione
    """
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size(0)
    index = torch.randperm(batch_size, device=x.device)
    mixed_x = lam * x + (1 - lam) * x[index]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam


# -----------------------------------
# Funzione: cutmix_data
#   Applica CutMix a un batch di immagini e label.
#   Sovrappone una regione casuale di un'immagine su un'altra.
# Params:
#   - x (torch.Tensor): batch di immagini (B, C, H, W)
#   - y (torch.Tensor): batch di etichette (B,)
#   - beta (float): parametro della Beta distribution
# Returns:
#   - x: batch di immagini modificate con CutMix
#   - y_a, y_b: etichette corrispondenti per la loss
#   - lam: frazione di immagine originale mantenuta
# -----------------------------------
def cutmix_data(x: torch.Tensor, y: torch.Tensor, beta: float = 1.0):
    """
    Applica CutMix a un batch.

    Args:
        x: batch di immagini, shape (B, C, H, W)
        y: batch di label, shape (B,)
        beta: parametro della Beta distribution per calcolare la porzione da tagliare/incollare

    Returns:
        x: immagini modificate in-place con CutMix
        y_a, y_b: coppie di label per il calcolo della loss
        lam: rapporto area immutata / area totale, usato per pesare la loss
    """
    lam = np.random.beta(beta, beta)
    batch_size, C, H, W = x.size()
    index = torch.randperm(batch_size, device=x.device)

    # Calcolo dimensioni riquadro da sostituire
    cut_rat = np.sqrt(1. - lam)
    cut_w, cut_h = int(W * cut_rat), int(H * cut_rat)
    cx, cy = np.random.randint(W), np.random.randint(H)
    bbx1 = max(cx - cut_w // 2, 0)
    bby1 = max(cy - cut_h // 2, 0)
    bbx2 = min(cx + cut_w // 2, W)
    bby2 = min(cy + cut_h // 2, H)

    # Sostituzione porzione centrale con quella di un altro campione
    x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]
    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))
    y_a, y_b = y, y[index]
    return x, y_a, y_b, lam

# --------------------------------------------------
# FUNZIONI DI TRAINING
# --------------------------------------------------

# -----------------------------------
# Funzione: train_one_epoch
#   Esegue una singola epoca di training, con supporto opzionale
#   a MixUp e/o CutMix nelle prime epoche.
# Params:
#   - model, optimizer: modello PyTorch e ottimizzatore
#   - dataloader: loader dei dati di training
#   - epoch: indice dellâ€™epoca corrente
#   - use_mixup/use_cutmix: attiva/disattiva le tecniche
#   - p_mixup/p_cutmix: probabilitÃ  di applicazione
#   - alpha/beta: parametri della distribuzione Beta
#   - criterion: funzione di loss
# Returns:
#   - average_loss: loss media sullâ€™epoca
#   - accuracy: accuracy media sullâ€™epoca
# -----------------------------------
def train_one_epoch(
    model: nn.Module,
    optimizer: torch.optim.Optimizer,
    dataloader: DataLoader,
    epoch: int,
    use_mixup: bool = True,
    mixup_epochs: int = 5,
    p_mixup: float = 0.5,
    alpha: float = 0.15,
    use_cutmix: bool = False,
    cutmix_epochs: int = 5,
    p_cutmix: float = 0.5,
    beta: float = 1.0,
    criterion: nn.Module = None
):
    """
    Esegue un'epoca di training, applicando opzionalmente MixUp/CutMix
    per le prime mixup_epochs/cutmix_epochs epoche con probabilitÃ  p_mixup/p_cutmix.

    Ritorna:
        loss medio sul batch, accuracy sul batch
    """
    model.train()
    running_loss, correct, total = 0.0, 0, 0

    for images, labels in dataloader:
        images = images.to(device)
        labels = labels.to(device).long()
        optimizer.zero_grad()

        # Applicazione MixUp o CutMix
        do_mix = use_mixup and (epoch < mixup_epochs) and (np.random.rand() < p_mixup)
        do_cut = use_cutmix and (epoch < cutmix_epochs) and (np.random.rand() < p_cutmix)

        if do_mix:
            images, y_a, y_b, lam = mixup_data(images, labels, alpha)
            outputs = model(images)
            loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)

        elif do_cut:
            images, y_a, y_b, lam = cutmix_data(images, labels, beta)
            outputs = model(images)
            loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)

        else:
            outputs = model(images)
            loss = criterion(outputs, labels)

        loss.backward()
        optimizer.step()

        # Accumula loss e conteggia accuracy
        running_loss += loss.item() * images.size(0)
        preds = outputs.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    return running_loss / total, correct / total

# -----------------------------------
# Funzione: validate
#   Valida il modello sul dataloader fornito senza calcolare i gradienti.
# Params:
#   - model: modello PyTorch
#   - dataloader: loader dei dati di validazione
#   - criterion: funzione di loss
# Returns:
#   - average_loss: loss media
#   - accuracy: accuracy complessiva
# -----------------------------------
def validate(model: nn.Module, dataloader: DataLoader, criterion: nn.Module):
    """
    Esegue la validazione su un dataloader (senza gradienti).
    Ritorna loss medio e accuracy sul batch.
    """
    model.eval()
    running_loss, correct, total = 0.0, 0, 0

    with torch.no_grad():
        for images, labels in dataloader:
            images = images.to(device)
            labels = labels.to(device).long()
            outputs = model(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item() * images.size(0)
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    return running_loss / total, correct / total

# -----------------------------------
# Funzione: train_model
#   Ciclo completo di training con warm-up iniziale, MixUp/CutMix,
#   scheduler, label smoothing ed early stopping.
# Params:
#   - model, optimizer, scheduler: oggetti PyTorch
#   - real_train_loader: dataloader con dati reali
#   - combined_train_loader: dataloader con dati reali + sintetici
#   - val_loader: dataloader per la validazione
#   - num_epochs, warmup_epochs: numero totale e di warm-up
#   - use_mixup/use_cutmix: attiva MixUp/CutMix
#   - early_stopping_patience: epoche senza miglioramento tollerate
#   - label_smoothing: livello di smoothing nella loss
# Returns:
#   - history (dict): metriche per loss e accuracy su train/val
# -----------------------------------
def train_model(
    model: nn.Module,
    optimizer: torch.optim.Optimizer,
    scheduler: torch.optim.lr_scheduler._LRScheduler,
    real_train_loader: DataLoader,
    combined_train_loader: DataLoader,
    val_loader: DataLoader,
    num_epochs: int = 20,
    warmup_epochs: int = 5,
    use_mixup: bool = True,
    mixup_epochs: int = 5,
    p_mixup: float = 0.5,
    alpha: float = 0.15,
    use_cutmix: bool = False,
    cutmix_epochs: int = 5,
    p_cutmix: float = 0.5,
    beta: float = 1.0,
    early_stopping_patience: int = 3,
    label_smoothing: float = 0.0
):
    """
    Funzione principale di training:
    - utilizza real_train_loader per le prime warmup_epochs,
      poi usa combined_train_loader (con dati sintetici).
    - applica MixUp/CutMix se richiesto.
    - esegue early stopping su val loss.

    Ritorna un dizionario con:
      - best_val_acc: miglior accuracy di validazione raggiunta
      - train_losses, val_losses: liste delle loss per epoca
      - train_accs, val_accs: liste delle accuracy per epoca
    """
    # Scelta criterio con o senza label smoothing
    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing) \
                if label_smoothing > 0 else nn.CrossEntropyLoss()

    history = {"train_losses": [], "val_losses": [], "train_accs": [], "val_accs": []}
    best_val_acc, best_val_loss = 0.0, float('inf')
    epochs_no_improve = 0

    for epoch in range(num_epochs):
        start = time.time()

        # Warm-up sulle epoche iniziali con dataset reale
        current_loader = real_train_loader if epoch < warmup_epochs else combined_train_loader

        tr_loss, tr_acc = train_one_epoch(
            model, optimizer, current_loader, epoch,
            use_mixup, mixup_epochs, p_mixup, alpha,
            use_cutmix, cutmix_epochs, p_cutmix, beta,
            criterion
        )
        val_loss, val_acc = validate(model, val_loader, criterion)

        # Aggiornamento scheduler se presente
        if scheduler is not None:
            scheduler.step(val_loss)

        # Registrazione metriche
        history["train_losses"].append(tr_loss)
        history["val_losses"].append(val_loss)
        history["train_accs"].append(tr_acc)
        history["val_accs"].append(val_acc)

        print(f"[Epoch {epoch+1}/{num_epochs}] "
              f"Tr L: {tr_loss:.4f} | Tr A: {tr_acc:.4f} | "
              f"Val L: {val_loss:.4f} | Val A: {val_acc:.4f} "
              f"({time.time()-start:.1f}s)")

        # Early stopping basato sulla validation loss
        if val_loss < best_val_loss:
            best_val_loss, best_val_acc, epochs_no_improve = val_loss, val_acc, 0
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= early_stopping_patience:
                print(f"Early stopping dopo {epoch+1} epoche senza miglioramenti.")
                break

    return {
        "best_val_acc": best_val_acc,
        "train_losses": history["train_losses"],
        "val_losses":   history["val_losses"],
        "train_accs":   history["train_accs"],
        "val_accs":     history["val_accs"]
    }

"""###Workflow"""

# numero di classi dal dataset combinato (stesso per entrambi i modelli)
num_classes = len(train_dataset.classes)

# creazione modelli e trasferimento su device
model_original = timm.create_model("efficientnet_b0", pretrained=True, num_classes=num_classes).to(device)
model_combined = timm.create_model("efficientnet_b0", pretrained=True, num_classes=num_classes).to(device)

# ottimizzatori e scheduler
opt_orig = optim.AdamW(model_original.parameters(), lr=1e-4)
opt_comb = optim.AdamW(model_combined.parameters(), lr=1e-4)
sch_orig = optim.lr_scheduler.ReduceLROnPlateau(opt_orig, mode='min', factor=0.5, patience=2, verbose=True)
sch_comb = optim.lr_scheduler.ReduceLROnPlateau(opt_comb, mode='min', factor=0.5, patience=2, verbose=True)

# Training sul dataset ORIGINALE
print(">>> TRAINING MODELLO ORIGINALE")
train_res_original = train_model(
    model_original, opt_orig, sch_orig,
    train_loader,         # solo immagini originali
    train_loader,     # train_loader, se non ci sono immagini sintetiche
    val_loader,
    num_epochs=10,
    warmup_epochs=3,
    use_mixup=False,
    use_cutmix=True,
    early_stopping_patience=3
)
print(f"Best Val Acc (Originale): {train_res_original['best_val_acc']:.4f}\n")

"""####**Analisi dell'Addestramento sul Dataset Originale**
1. **Inizio dell'Addestramento (Epoca 1)**:

  All'inizio l'errore di training Ã¨ molto alto e l'accuratezza Ã¨ bassa, come atteso per un modello non ancora addestrato. Tuttavia, la validation accuracy parte giÃ  da un valore moderato (70%), indicando che il modello riesce a cogliere alcuni pattern rilevanti fin dalle prime fasi di addestramento.

2. **Rapido Miglioramento nelle Prime Epoche (Epoche 2-3)**:

  Il modello dimostra un miglioramento significativo giÃ  nelle prime tre epoche, riducendo rapidamente la perdita e aumentando in maniera consistente l'accuratezza sia sul training che sulla validazione.

3. **Convergenza e Stabilizzazione (Epoche 4-10)**:

  Dalla quarta epoca in poi, il modello continua a migliorare in modo piÃ¹ graduale:

  - Train Accuracy cresce stabilmente fino a raggiungere il 100% all'ultima epoca.

  - Validation Accuracy sale progressivamente fino a stabilizzarsi intorno al 90-91%.

  - Train Loss crolla drasticamente (da 1.1819 a 0.0205) e la Validation Loss si riduce fino a circa 0.3175.

  L'andamento delle curve suggerisce che il modello continua a raffinare la sua capacitÃ  predittiva senza cadere in overfitting evidente.

4. **Prestazioni Finali e Tempi di Esecuzione**:

- Miglior Validation Accuracy raggiunta: 90.76%.

- I tempi di esecuzione per epoca oscillano tra 32 e 34 secondi, mantenendosi costanti e indicando un processo di addestramento stabile.

  ####  **Considerazioni Finali**

  **Apprendimento Rapido**: Il modello mostra un apprendimento costante, passando da una situazione iniziale di elevato errore a performance solide, senza bisogno di molte epoche.

  **Buona Generalizzazione**: La Validation Accuracy aumenta in modo proporzionato rispetto al Training Accuracy, senza mostrare divari significativi o segnali di overfitting, almeno nei limiti delle 10 epoche considerate.

  **Efficienza del Processo di Training**: L'addestramento Ã¨ stato fluido e regolare, sia in termini di prestazioni computazionali che di andamento delle metriche.

In sintesi, il modello addestrato sul dataset originale ha raggiunto buoni livelli di accuratezza e stabilitÃ , risultando promettente per l'applicazione pratica nella fase successiva di validazione o test.
"""

# Training sul dataset COMBINATO (reale + sintetico)
print(">>> TRAINING MODELLO COMBINATO")
train_res_combined = train_model(
    model_combined, opt_comb, sch_comb,
    train_loader,                 # immagini originali
    combined_train_loader,    # immagini combinate (originali + sintetiche)
    combined_val_loader,
    num_epochs=20,
    warmup_epochs=3,         # addestramento iniziale solo con immagini reali
    use_mixup=False, mixup_epochs=3, p_mixup=0.2, alpha=0.1,
    use_cutmix=True, cutmix_epochs=5, p_cutmix=0.5, beta=1.0,
    early_stopping_patience=3,
    label_smoothing=0.05
)
print(f"Best Val Acc (Combinato): {train_res_combined['best_val_acc']:.4f}\n")

"""#### **Analisi dell'Addestramento sul Dataset Combinato**

1. **Fase Iniziale (Epoche 1â€“3)**  
   - **Epoca 1**: GiÃ  dalla prima epoca il modello combinato mostra unâ€™ottima capacitÃ  di generalizzazione, grazie al dataset esteso e alle immagini sintetiche: la validation accuracy Ã¨ molto alta pur partendo da un training loss ancora elevato.  
   - **Epoca 2**: Rapido miglioramento della performance, confermando che il modello sfrutta efficacemente la varietÃ  introdotta dalle nuove immagini.  
   - **Epoca 3**: Un leggero aumento del training loss e una piccola flessione della train accuracy suggeriscono il â€œrumoreâ€ aggiuntivo del MixUp iniziale, ma la validation accuracy raggiunge subito un picco molto alto (95.15%), indice di ottima robustezza.

2. **Consolidamento (Epoche 4â€“6)**  
     Il passaggio dalla fase di warm-up (solo dati reali) a quella combinata porta a un drastico calo del training loss, mentre la validation accuracy rimane molto elevata.

3. **Saturazione e Early Stopping (Epoche 7â€“12)**  
   Dalla **Epoca 7** in poi il training loss si assesta attorno a 0.45, con train accuracy costantemente ~100%.

   La validation loss oscilla leggermente intorno a 0.55, mentre la validation accuracy va oltre il 96%, raggiungendo il picco (97.50%) all'epoca 9.  

   **Early stopping** al 12Â° ciclo, nonostante lâ€™assenza di peggioramenti significativi, evita un potenziale overfitting su dati rumorosi.

4. **Risultato Finale**  
  Il modello combinato ottiene performance estremamente solide e stabili, con picchi di validazione superiori a quelli del solo dataset originale.

#### **Considerazioni Finali**

- **Elevata Generalizzazione**: Lâ€™aggiunta di captioning + synth images + MixUp ha reso il modello molto robusto, capace di mantenere una validation accuracy superiore al 96% su un dataset semplificato.  
- **Gestione del Rumore**: Le leggere oscillazioni nelle prime epoche sono indice dellâ€™effetto regolarizzante delle tecniche di augmentation, che rallentano inizialmente lâ€™apprendimento ma migliorano la stabilitÃ  a lungo termine.  
- **Ottimo Candidato per Ensemble**: Grazie alla ridotta varianza delle predizioni e alle performance quasi â€œpiatteâ€ in validazione, questo modello Ã¨ ideale come componente in un ensemble per massimizzare lâ€™accuratezza complessiva (come confermato dal risultato finale di ensemble â‰ˆ 99.87%).  
- **Efficienza del Training**: Tempi epoca costanti (~33 s) e early stopping efficace rendono il processo ben ottimizzato per successive iterazioni.

In sintesi, lâ€™addestramento sul dataset combinato ha portato a un modello estremamente solido e generalizzabile, perfetto per integrare le sue predizioni in un ensemble ad alte prestazioni.

##Valutazioni

###Funzioni

####Risultati
"""

# -----------------------------------
# Funzione: evaluate_single_model
#   Valuta un singolo modello PyTorch sul dataloader.
# Params:
#   - model: modello in modalitÃ  eval
#   - dataloader: batch di immagini e label
#   - device: dispositivo su cui eseguire (es. 'cuda')
# Returns:
#   - labels: array NumPy con label reali
#   - preds: array NumPy con predizioni del modello
# -----------------------------------
def evaluate_single_model(model: nn.Module, dataloader: DataLoader, device: torch.device) -> tuple[np.ndarray, np.ndarray]:
    """
    Valuta un singolo modello su un dataloader.

    Args:
        model: modello PyTorch in modalitÃ  eval.
        dataloader: DataLoader da cui estrarre batch di (immagini, label).
        device: dispositivo su cui spostare i dati (es. 'cuda' o 'cpu').

    Returns:
        labels: array NumPy delle label vere concatenate.
        preds:  array NumPy delle predizioni del modello concatenate.
    """
    model.to(device).eval()
    all_labels, all_preds = [], []

    with torch.no_grad():
        for images, labels in dataloader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            _, preds = torch.max(outputs, dim=1)

            all_labels.append(labels.cpu().numpy())
            all_preds .append(preds.cpu().numpy())

    # Unione batch in due array piatti
    return np.concatenate(all_labels), np.concatenate(all_preds)

# -----------------------------------
# Funzione: ensemble_predict
#   Esegue ensemble di piÃ¹ modelli sommando le softmax pesate.
# Params:
#   - models: lista di modelli PyTorch
#   - dataloader: loader dei dati di test
#   - weights: pesi associati ai modelli nellâ€™ensemble (opzionali)
# Returns:
#   - labels: tensor di label reali
#   - preds: tensor di predizioni dellâ€™ensemble
# -----------------------------------
def ensemble_predict(models: list[nn.Module],
                     dataloader: DataLoader,
                     device: torch.device,
                     weights: list[float] = None) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Esegue un ensemble di modelli con weighted averaging delle softmax.

    Args:
        models: lista di modelli PyTorch.
        dataloader: DataLoader di test/val.
        device: dispositivo su cui eseguire.
        weights: lista di pesi di lunghezza pari a len(models);
                 se None, usa pesi uniformi.

    Returns:
        all_labels: tensor CPU delle label vere concatenate.
        all_preds:  tensor CPU delle predizioni ensemble concatenate.
    """
    if weights is None:
        weights = [1.0 / len(models)] * len(models)

    # Trasferimento modelli su device
    for m in models:
        m.to(device).eval()

    all_preds, all_labels = [], []

    with torch.no_grad():
        for images, labels in dataloader:
            images = images.to(device)

            # costruzione output ensemble come somma pesata delle softmax
            ensemble_out = None
            for model, w in zip(models, weights):
                probs = F.softmax(model(images), dim=1)
                ensemble_out = probs * w if ensemble_out is None else ensemble_out + probs * w

            preds = ensemble_out.argmax(dim=1)

            all_preds .append(preds.cpu())
            all_labels.append(labels)

    return torch.cat(all_labels), torch.cat(all_preds)

# -----------------------------------
# Funzione: evaluate_model
#   Valuta un modello o un ensemble sul dataloader e calcola metriche sklearn.
# Params:
#   - models: modello singolo o lista di modelli
#   - dataloader: dati da valutare
#   - device: dispositivo di calcolo
#   - ensemble: se True, applica ensemble_predict
#   - ensemble_weights: pesi per il soft voting (opzionale)
# Returns:
#   - dizionario con report, accuracy, precision, recall, f1, e matrice di confusione
# -----------------------------------
def evaluate_model(models, dataloader: DataLoader,
                   device: torch.device,
                   ensemble: bool = False,
                   ensemble_weights: list[float] = None) -> dict:
    """
    Valuta un singolo modello o un ensemble sul dataloader e restituisce le metriche.

    Args:
        models:
          - se ensemble=False, un singolo modello (nn.Module);
          - se ensemble=True, lista di modelli.
        dataloader: DataLoader di test/val.
        device: dispositivo di calcolo.
        ensemble: se True, usa ensemble_predict, altrimenti evaluate_single_model.
        ensemble_weights: pesi per lâ€™ensemble (se ensemble=True).

    Returns:
        dict con:
          - 'report': stringa di classification_report sklearn,
          - 'accuracy': accuracy globale,
          - 'precision_macro': precision macro,
          - 'recall_macro': recall macro,
          - 'f1_macro': f1-score macro,
          - 'confusion_matrix': 2D numpy array.
    """
    # Raccolta labels e preds
    if ensemble:
        labels, preds = ensemble_predict(models, dataloader, device, weights=ensemble_weights)
        labels = labels.cpu().numpy()
        preds  = preds .cpu().numpy()
    else:
        labels, preds = evaluate_single_model(models, dataloader, device)

    # Calcolo metriche
    report = classification_report(labels, preds)
    acc    = accuracy_score(labels, preds)
    prec   = precision_score(labels, preds, average="macro")
    rec    = recall_score(labels, preds, average="macro")
    f1     = f1_score(labels, preds, average="macro")
    cm     = confusion_matrix(labels, preds)

    return {
        "report": report,
        "accuracy": acc,
        "precision_macro": prec,
        "recall_macro": rec,
        "f1_macro": f1,
        "confusion_matrix": cm
    }

"""####Matrice di confusione"""

# -----------------------------------
# Funzione: plot_confusion
#   Visualizza una matrice di confusione su un asse Matplotlib.
# Params:
#   - ax: oggetto Axes su cui disegnare
#   - cm: matrice di confusione (2D array)
#   - title: titolo del grafico
#   - class_names: etichette di classe (opzionali)
# -----------------------------------
def plot_confusion(ax, cm, title, class_names=None):
    """
    Disegna una matrice di confusione su un asse Matplotlib.

    Args:
        ax: Matplotlib Axes su cui disegnare.
        cm: 2D array (confusion matrix) di interi.
        title: titolo da assegnare al grafico.
        class_names: lista di etichette delle classi (opzionale).
                     Se fornita, verranno usate per i tick degli assi.
    """
    # Mostrare matrice come immagine
    im = ax.imshow(cm, interpolation='nearest', cmap='Blues')
    ax.set_title(title, fontsize=14)

    # Aggiunta colorbar
    cbar = ax.figure.colorbar(im, ax=ax)
    cbar.ax.set_ylabel("Count", rotation=-90, va="bottom")

    # Impostare ticks e label
    if class_names is not None:
        ax.set_xticks(range(len(class_names)))
        ax.set_yticks(range(len(class_names)))
        ax.set_xticklabels(class_names, rotation=45, ha='right')
        ax.set_yticklabels(class_names)
    else:
        # se non ci sono nomi, numeri di default
        ax.set_xticks(range(cm.shape[1]))
        ax.set_yticks(range(cm.shape[0]))

    # Annotazioni numero di esempi in ciascuna cella
    thresh = cm.max() / 2
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(
                j, i, format(cm[i, j], 'd'),
                ha='center', va='center',
                color='white' if cm[i, j] > thresh else 'black'
            )

    # Etichette assi
    ax.set_ylabel('True label', fontsize=12)
    ax.set_xlabel('Predicted label', fontsize=12)
    ax.tick_params(axis='both', which='major', labelsize=10)
    ax.grid(False)

"""####Consolidamento dei risultati"""

# -----------------------------------------------
# Funzione: evaluate_ensemble
#   Valuta un ensemble di modelli sul set di test.
# Params:
#   - models: lista di modelli PyTorch
#   - loader: DataLoader di test/hold-out
#   - device: 'cuda' o 'cpu'
#   - weights: lista di pesi per i modelli (opzionale)
# Returns:
#   - labels: array delle etichette vere
#   - preds: array delle predizioni finali
#   - logits: array delle probabilitÃ  combinate
# -----------------------------------------------
def evaluate_ensemble(models, loader, device, weights=None):
    """
    Valuta un ensemble di modelli sul hold-out set, restituendo le etichette vere,
    le predizioni e i logit combinati.

    Args:
        models (list): lista di modelli PyTorch giÃ  istanziati.
        loader (DataLoader): DataLoader per il set di test/hold-out.
        device (torch.device): dispositivo su cui eseguire (es. "cuda" o "cpu").
        weights (list, optional): pesi da applicare a ciascun modello. Se None,
                                  si usa peso 1/len(models) per ogni modello.

    Returns:
        labels (np.ndarray): array delle etichette vere.
        preds  (np.ndarray): array delle etichette predette dallâ€™ensemble.
        logits  (np.ndarray): array dei logit (probabilitÃ  pesate) per ogni esempio.
    """
    ensemble_models = [m.to(device).eval() for m in models]

    all_logits = []
    all_labels = []

    with torch.no_grad():
        for imgs, labels in loader:
            imgs   = imgs.to(device)
            labels = labels.to(device)

            # Costruzione output combinato: somma pesata delle softmax
            combined_probs = None
            for idx, mdl in enumerate(ensemble_models):
                # determinare peso: uniforme se non specificato
                w = weights[idx] if (weights is not None and idx < len(weights)) else 1.0 / len(ensemble_models)
                logits = mdl(imgs)
                probs  = F.softmax(logits, dim=1) * w

                # somma iterativa
                combined_probs = probs if combined_probs is None else combined_probs + probs

            all_logits.append(combined_probs.cpu())
            all_labels.append(labels.cpu())

    # Concatenare batch-wise
    all_logits = torch.cat(all_logits)  # shape: [N, num_classes]
    all_labels = torch.cat(all_labels)  # shape: [N]

    # Predizioni finali: argmax sui logit combinati
    preds  = all_logits.argmax(dim=1).cpu().numpy()
    labels = all_labels.cpu().numpy()

    return labels, preds, all_logits.numpy()

"""####Grafici di comparazione"""

# -----------------------------------------------------------
# Funzione: plot_training_curves
#   Mostra lâ€™andamento di loss e accuracy in training/validation.
# Params:
#   - train_losses: lista dei valori di loss su training
#   - val_losses: lista dei valori di loss su validation
#   - train_accs: lista delle accuracy su training
#   - val_accs: lista delle accuracy su validation
#   - model_name: nome del modello per il titolo del plot
# -----------------------------------------------------------
def plot_training_curves(train_losses, val_losses, train_accs, val_accs, model_name="Modello"):
    """
    Disegna fianco a fianco lâ€™andamento di loss e accuracy in training e validation.

    Args:
        train_losses (list[float]): loss calcolata su training per ogni epoca.
        val_losses   (list[float]): loss calcolata su validation per ogni epoca.
        train_accs   (list[float]): accuracy su training per ogni epoca.
        val_accs     (list[float]): accuracy su validation per ogni epoca.
        model_name (str): etichetta da mostrare nei titoli dei plot.
    """
    epochs = range(1, len(train_losses) + 1)
    plt.figure(figsize=(12, 5))

    # Subplot per la loss
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_losses, label='Train Loss')  # curva di loss in training
    plt.plot(epochs, val_losses,   label='Val Loss')    # curva di loss in validation
    plt.title(f'{model_name} â€“ Andamento della Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # Subplot per lâ€™accuracy
    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_accs, label='Train Acc')    # curva di accuracy in training
    plt.plot(epochs, val_accs,   label='Val Acc')      # curva di accuracy in validation
    plt.title(f"{model_name} â€“ Andamento dell'Accuratezza")
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

# -----------------------------------------------------------
# Funzione: plot_classification_metrics_by_model
#   Grafico a barre di precision, recall, F1 per ciascuna classe.
# Params:
#   - y_true: etichette vere
#   - y_pred: etichette predette
#   - class_names: lista dei nomi delle classi
#   - model_name: nome del modello per il titolo del plot
# -----------------------------------------------------------
def plot_classification_metrics_by_model(y_true, y_pred, class_names, model_name):
    """
    Crea un grafico a barre affiancate di precision, recall e F1-score per ogni classe.

    Args:
        y_true      (array-like): etichette vere.
        y_pred      (array-like): etichette predette.
        class_names (list[str]): nomi delle classi, nellâ€™ordine corretto.
        model_name  (str): nome del modello per il titolo del plot.
    """
    # Ottere dict di metriche dal classification_report
    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)

    # Estrazione precision, recall e F1 per ogni classe
    precision = [report[cls]['precision'] for cls in class_names]
    recall    = [report[cls]['recall']    for cls in class_names]
    f1_score  = [report[cls]['f1-score']  for cls in class_names]

    x = np.arange(len(class_names))
    width = 0.25

    plt.figure(figsize=(10, 6))
    # Barre per precision, recall, f1
    plt.bar(x - width, precision, width, label='Precision')
    plt.bar(x,       recall,    width, label='Recall')
    plt.bar(x + width, f1_score, width, label='F1-Score')

    plt.ylabel('Valori')
    plt.xlabel('Classi')
    plt.title(f'Metriche di Classificazione per Classe â€“ {model_name}')
    plt.xticks(x, class_names, rotation=45, ha='right')
    plt.ylim(0, 1.1)
    plt.legend()
    plt.grid(axis='y')
    plt.tight_layout()
    plt.show()

# -----------------------------------------------------------
# Funzione: plot_predictions_by_model
#   Visualizza esempi di predizioni corrette ed errate.
# Params:
#   - model: modello PyTorch
#   - dataloader: DataLoader del set di test
#   - class_names: nomi delle classi
#   - device: dispositivo per lâ€™inferenza
#   - model_name: nome del modello per il titolo
#   - num_images: numero totale di immagini da mostrare
# Returns:
#   - correct_images: lista di tuple immagini corrette
#   - incorrect_images: lista di tuple immagini errate
# -----------------------------------------------------------
def plot_predictions_by_model(model, dataloader, class_names, device, model_name, num_images=12):
    """
    Mostra un grid di immagini con vere vs predette, metÃ  corrette e metÃ  errate.

    Args:
        model        (nn.Module): modello da valutare.
        dataloader   (DataLoader): loader del dataset (solitamente test_loader).
        class_names  (list[str]): lista dei nomi di classe.
        device       (torch.device): device su cui far girare il modello.
        model_name   (str): nome del modello (per il titolo).
        num_images   (int): numero totale di immagini da mostrare (corrette+errate).
    Returns:
        correct_images   (list): tuple (img_array, true_label, pred_label) corrette.
        incorrect_images (list): tuple (img_array, true_label, pred_label) errate.
    """
    model.to(device).eval()
    correct_images, incorrect_images = [], []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            preds   = outputs.argmax(dim=1)

            # Iterazione sul batch e separa corrette/errate fino a num_images
            for i in range(inputs.size(0)):
                if len(correct_images) + len(incorrect_images) >= num_images:
                    break

                img = inputs[i].cpu().permute(1, 2, 0).numpy()
                true_label = class_names[labels[i].item()]
                pred_label = class_names[preds[i].item()]

                if preds[i] == labels[i] and len(correct_images) < num_images // 2:
                    correct_images.append((img, true_label, pred_label))
                elif preds[i] != labels[i] and len(incorrect_images) < num_images // 2:
                    incorrect_images.append((img, true_label, pred_label))

            if len(correct_images) + len(incorrect_images) >= num_images:
                break

    # Plot in una griglia 3Ã—4
    fig, axes = plt.subplots(3, 4, figsize=(16, 10))
    axes = axes.flatten()

    for idx, (img, true_lbl, pred_lbl) in enumerate(correct_images + incorrect_images):
        axes[idx].imshow(img)
        color = 'green' if true_lbl == pred_lbl else 'red'
        axes[idx].set_title(f'True: {true_lbl}\nPred: {pred_lbl}', color=color)
        axes[idx].axis('off')

    plt.suptitle(f"Predizioni â€“ {model_name}", fontsize=14)
    plt.tight_layout()
    plt.show()

    return correct_images, incorrect_images

# -----------------------------------------------------------
# Funzione: plot_augmented_samples
#   Mostra esempi da un DataLoader (con eventuali augmentations).
# Params:
#   - dataloader: DataLoader da cui prendere il batch
#   - class_names: nomi delle classi
#   - dataset_name: etichetta del dataset (es. "Originale")
#   - num_images: numero di immagini da visualizzare
# Returns:
#   - images: batch di immagini
#   - labels: etichette corrispondenti
# -----------------------------------------------------------
def plot_augmented_samples(dataloader, class_names, dataset_name="Originale", num_images=8):
    """
    Mostra alcuni esempi di immagini prelevate dal DataLoader (con augmentation applicata).

    Args:
        dataloader    (DataLoader): loader da cui estrarre un batch.
        class_names   (list[str]): nomi delle classi per i titoli.
        dataset_name  (str): etichetta da mostrare nel titolo.
        num_images    (int): numero di immagini da visualizzare.
    Returns:
        images (Tensor): batch di immagini ritornato dal loader.
        labels (Tensor): batch di label corrispondenti.
    """
    images, labels = next(iter(dataloader))

    fig, axes = plt.subplots(1, num_images, figsize=(20, 3))
    for idx in range(num_images):
        img = images[idx].permute(1, 2, 0).numpy()  # CÃ—HÃ—W â†’ HÃ—WÃ—C
        lbl_idx = labels[idx].item() if isinstance(labels[idx], torch.Tensor) else int(labels[idx])
        axes[idx].imshow(img)
        axes[idx].set_title(class_names[lbl_idx])
        axes[idx].axis('off')

    plt.suptitle(f"Augmentations â€“ {dataset_name}", fontsize=14)
    plt.tight_layout()
    plt.show()

    return images, labels

# -----------------------------------------------------------
# Funzione: plot_model_comparison
#   Confronta modelli su Accuracy e F1 Macro tramite bar chart.
# Params:
#   - model_names: nomi dei modelli
#   - accuracies: lista delle accuracy
#   - f1_scores: lista dei punteggi F1 Macro
# Returns:
#   - fig, ax: oggetti Matplotlib per ulteriori modifiche
# -----------------------------------------------------------
def plot_model_comparison(model_names, accuracies, f1_scores):
    """
    Confronta performance di piÃ¹ modelli con un bar chart di Accuracy vs F1 Macro.

    Args:
        model_names (list[str]): nomi dei modelli (asse x).
        accuracies  (list[float]): accuracy corrispondenti.
        f1_scores   (list[float]): F1 Macro corrispondenti.
    Returns:
        fig, ax: figure e assi Matplotlib, utili se si vuole salvare o ulteriormente personalizzare.
    """
    x = np.arange(len(model_names))
    width = 0.35

    fig, ax = plt.subplots(figsize=(10, 6))
    bars1 = ax.bar(x - width/2, accuracies, width, label='Accuracy')
    bars2 = ax.bar(x + width/2, f1_scores, width, label='F1 Macro')

    ax.set_ylabel('Score')
    ax.set_title('Confronto delle Performance tra i Modelli')
    ax.set_xticks(x)
    ax.set_xticklabels(model_names, rotation=15)
    ax.set_ylim(0, 1.05)

    # Annotazioni sopra ogni barra
    for bar in bars1 + bars2:
        h = bar.get_height()
        ax.annotate(f'{h:.3f}',
                    xy=(bar.get_x() + bar.get_width()/2, h),
                    xytext=(0, 3),
                    textcoords='offset points',
                    ha='center', va='bottom')

    ax.legend()
    ax.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()

    return fig, ax

"""###Workflow

####Risultati
"""

# Modello Originale
res_orig = evaluate_model(
    models=model_original,
    dataloader=test_loader,
    device=device,
    ensemble=False
)
print("=== Modello Originale ===")
print(res_orig["report"])

"""##### **Analisi Complessiva**

Il modello originale si comporta in modo eccellente sul test set, raggiungendo unâ€™**accuratezza complessiva del 97%**. Le medie **macro** e **weighted** (entrambi intorno a 0.97) confermano che le prestazioni sono uniformi tra le classi, senza eccessivi squilibri.

##### **Punti di Forza**

Numerose classi(esempio 2, 7, 9, 10 e molte altre) ottengono **precisione, recall e F1-score pari a 1.00**, segno che il modello riconosce perfettamente questi soggetti senza confonderli. Anche classi storicamente piÃ¹ difficili mostrano F1 superiori a 0.95, dimostrando una capacitÃ  discriminativa molto solida.

##### **Aree di Miglioramento**

Un paio di classi meritano attenzione:

- **Classe 0**: recall ridotto (0.76) nonostante una buona precisione, indica che molti esempi reali di questa classe non vengono rilevati.  
- **Classe 26**: pur avendo F1 vicino a 0.89, il recall al 0.95 suggerisce qualche mancanza nel rilevamento.

In generale, le classi con supporto piÃ¹ basso (meno campioni disponibili) tendono a presentare metriche leggermente piÃ¹ instabili.

##### **Considerazioni Finali**

Nel complesso, il modello Ã¨ **robusto** e **bilanciato**, con quasi tutte le classi riconosciute quasi perfettamente. Per avvicinarsi al 100% di recall anche sulle classi piÃ¹ critiche, si potrebbero applicare:

- **Data augmentation mirata** su classi deboli (es. rotazioni, ritagli specifici)  
- **Ponderazione della loss** per dare piÃ¹ peso agli esempi meno frequenti  
- **Fine-tuning** focalizzato solo sulle classi con recall inferiore

Questi interventi, uniti allâ€™uso dellâ€™ensemble con il modello combinato, dovrebbero consentire un ulteriore passo avanti verso prestazioni davvero uniformi su tutte le 37 classi.
"""

# Modello Combinato
res_comb = evaluate_model(
    models=model_combined,
    dataloader=test_loader,
    device=device,
    ensemble=False
)
print("=== Modello Combinato ===")
print(res_comb["report"])

"""##### **Analisi Complessiva**

Il modello combinato raggiunge unâ€™**accuratezza perfetta (99%)** sul test set e mostra medie **macro** e **weighted** ugualmente a **0.99** per precision, recall e F1-score. Questi risultati indicano una capacitÃ  di classificazione praticamente impeccabile su tutte le 37 classi.

##### **Punti di Forza**

- Per la quasi totalitÃ  delle classi, precision, recall e F1-score sono pari a 1.00, segno che il modello non confonde quasi mai esempi reali nÃ© produce falsi positivi.  
- Anche classi con supporto ridotto o storicamente piÃ¹ critiche mantengono F1 sopra 0.95, confermando lâ€™efficacia del data augmentation e delle immagini sintetiche nel rafforzare la generalizzazione.

##### **Piccole Discrepanze Residue**

- **Classe 21** (precision 1.00, recall 0.93): qualche campione non Ã¨ riconosciuto, probabilmente a causa di una leggera variabilitÃ  visiva.  
- **Classe 5** (recall 0.94) e **classe 11** (precision 0.95) evidenziano margini di miglioramento nei casi piÃ¹ ambigui, ma gli errori sono isolati e di lieve entitÃ .

##### **Considerazioni Finali**

Lâ€™integrazione di immagini sintetiche generate da captioning e Stable Diffusion, unita alle tecniche di MixUp e CutMix, ha prodotto un modello estremamente robusto e bilanciato. Le performance uniformi su tutte le classi dimostrano che lâ€™approccio di estendere il dataset con esempi realistici e varianti di caption ha massimizzato la capacitÃ  discriminativa senza sacrificare la precisione.

Il prossimo step potrebbe essere unâ€™analisi qualitativa degli errori residui per valutare possibili fine-tuning mirati sulle pochissime classi non ancora perfette, ma nel complesso il modello combinato appare giÃ  pronto per applicazioni pratiche su larga scala.  
"""

# Ensemble dei due modelli
res_ens = evaluate_model(
    models=[model_original, model_combined],
    dataloader=test_loader,
    device=device,
    ensemble=True,
    ensemble_weights=[0.5, 0.5]
)
print("=== Ensemble ===")
print(res_ens["report"])

"""##### **Analisi Complessiva**

Lâ€™ensemble raggiunge unâ€™accuratezza complessiva di circa **99%**, con medie **macro** e **weighted** pari a **0.99** per precision, recall e F1-score. Questo testimonia una straordinaria uniformitÃ  nelle prestazioni su tutte le 37 classi.

##### **Punti di Forza**

- **Quasi tutte le classi** ottengono punteggi perfetti (1.00) in precision, recall e F1-score, dimostrando che la combinazione dei modelli elimina quasi ogni errore residuo.  
- Anche le classi con meno esempi (ad es. alcune con supporto inferiore a 15) vengono riconosciute senza alcuna difficoltÃ , segno di unâ€™ottima generalizzazione.

##### **Aree di Attenzione**

- **Classe 1**: soffre ancora un poâ€™ con un recall di **0.95** (F1-score 0.95), suggerendo che alcuni esempi particolarmente variabili non vengono sempre intercettati.  
- **Classe 20, 34 e 35**: mostrano un recall di **0.96** (F1-score 0.98), indice di lievi ambiguitÃ  visive che possono generare falsi negativi.  
- **Classe 18**: con precisione a **0.94**, evidenzia rari casi di confusione, ma lâ€™impatto sul risultato complessivo Ã¨ minimo.

##### **Considerazioni Finali**

Lâ€™approccio ensemble sfrutta al meglio i punti di forza dei modelli originali e combinati, mitigando le loro singole debolezze e producendo previsioni di qualitÃ  quasi perfetta. Per unâ€™ulteriore spinta verso lâ€™eccellenza, si potrebbe condurre unâ€™analisi qualitativa degli errori residui sulle classi 0 e 20, ma nel complesso il sistema Ã¨ giÃ  pronto per essere impiegato in applicazioni reali di classificazione complessa.

####Matrice di confusione(Heatmap)
"""

# Estrazione delle confusion matrix per ciascun modello:
#   - originale, combinato (con immagini sintetiche), e ensemble
cm_orig = res_orig["confusion_matrix"]
cm_comb = res_comb["confusion_matrix"]
cm_ens  = res_ens["confusion_matrix"]

# Visualizzazione delle confusion matrix per ciascun modello
#    - In una griglia 2x2 (l'ultimo subplot viene disattivato)
fig, axs = plt.subplots(2, 2, figsize=(16, 14))
plot_confusion(axs[0, 0], cm_orig, "Confusion Matrix - Modello Originale")
plot_confusion(axs[0, 1], cm_comb, "Confusion Matrix - Modello Combinato")
plot_confusion(axs[1, 1], cm_ens,  "Confusion Matrix - Ensemble")

# Disattiva il subplot vuoto in basso a sinistra
axs[1, 0].axis('off')

plt.tight_layout()
plt.show()

"""##### **Analisi sulle Matrici di Confusione**

Lâ€™osservazione delle matrici di confusione conferma quanto emerso dalle metriche: il modello originale giÃ  si comporta bene, con la maggior parte delle predizioni concentrate sulla diagonale. Tuttavia permangono errori isolati in alcune classi, soprattutto quelle visivamente affini, che talvolta vengono confuse tra loro. Questo indica una discriminazione generalmente buona, ma ancora migliorabile nei casi di sovrapposizione semantica o visiva.

Con il modello combinato, addestrato su un dataset arricchito grazie a tecniche di data augmentation e immagini sintetiche, si vede chiaramente un salto di qualitÃ : quasi tutte le classi sono classificate correttamente senza alcuna confusione sistematica. Gli errori fuori diagonale diventano marginali e sporadici, segno che lâ€™arricchimento dei dati ha fornito esempi piÃ¹ vari e rappresentativi, rafforzando la capacitÃ  del modello di distinguere le classi piÃ¹ difficili.

Lâ€™ensemble consolida ulteriormente questo trend. Le matrici presentano una quasi totalitÃ  di vere positive in ogni riga, anche per le classi meno rappresentate, e gli errori residui sono rari e sparsi. Lâ€™approccio di combinare le probabilitÃ  dei due modelli riduce gli effetti di eventuali overfitting o punti ciechi specifici, garantendo previsioni estremamente stabili e affidabili.

##### **Conclusione comparativa**

Dal modello originale, con performance buone ma qualche confusione specifica, si passa al modello combinato, che sfrutta dati aggiuntivi per eliminare quasi tutti gli errori, fino allâ€™ensemble che ottimizza ulteriormente la generalizzazione. Questa progressione evidenzia come lâ€™arricchimento del dataset e la fusione delle predizioni di piÃ¹ modelli siano strategie molto efficaci per raggiungere performance quasi perfette in contesti di classificazione multi-classe complessa.

####Consolidamento dei risultati
"""

# --------------------------------------------------
# CREAZIONE DELLâ€™HOLD-OUT SET (10% DEL DATASET COMBINATO)
# L'hold-out verrÃ  usato solo per la valutazione finale, mai durante l'addestramento.
# --------------------------------------------------
holdout_ratio = 0.1
holdout_size = int(len(combined_dataset) * holdout_ratio)
trainval_size = len(combined_dataset) - holdout_size

# Suddivisione combined_dataset in train/val (90%) e hold-out finale (10%)
trainval_set, holdout_set = random_split(
    combined_dataset,
    [trainval_size, holdout_size],
    generator=torch.Generator().manual_seed(42)  # per riproducibilitÃ 
)

# DataLoader per lâ€™hold-out (batch size fisso, senza shuffle)
holdout_loader = DataLoader(holdout_set, batch_size=32, shuffle=False)

print(f"Hold-out creato: {holdout_size} immagini (~{holdout_ratio*100:.0f}%)")

# --------------------------------------------------
# VALUTAZIONE DELLâ€™ENSEMBLE SULLâ€™HOLD-OUT
# --------------------------------------------------
# Ottiene le etichette vere, le predizioni e le probabilitÃ  dal modello ensemble.
# Le probabilitÃ  predette da entrambi i modelli vengono combinate usando pesi specificati (0.5 ciascuno).
labels_hold, preds_hold, probs_hold = evaluate_ensemble(
    [model_original, model_combined],  # lista dei modelli da valutare
    holdout_loader,                   # DataLoader per il dataset hold-out
    device,                           # dispositivo (CPU/GPU)
    weights=[0.5, 0.5]                # pesi per la combinazione delle predizioni
)

# Calcolo e stampa accuratezza finale ensemble sul dataset hold-out
acc_hold = accuracy_score(labels_hold, preds_hold)
print(f"\nEnsemble su Hold-out â€” Accuracy: {acc_hold:.4f}\n")

# Stampa classification report dettagliato per ogni classe del dataset
print(classification_report(
    labels_hold,                     # etichette vere
    preds_hold,                      # predizioni fatte dall'ensemble
    target_names=train_dataset.classes  # nomi delle classi per il report
))

"""**Analisi dei Casi Non Perfetti**
Nonostante il punteggio complessivo sia praticamente perfetto, emergono due lievi discrepanze:

- **American Bulldog (supporto=10)**:
 - ***Recall***: 0.90 (2/10 esempi non rilevati)
 - ***Precision***: 1.00
 - ***F1-score***: 0.95

  Questo indica che tutte le predizioni fatte per â€œAmerican Bulldogâ€ sono corrette, ma il modello non ha riconosciuto in due casi la classe reale.

- **Saint Bernard (supporto=11)**:
 - ***Precision***: 0.92 (1 falso positivo)
 - ***Recall***: 1.00
 - ***F1-score***: 0.96

  Qui il modello ha classificato perfettamente tutti i casi reali, ma ha assegnato erroneamente unâ€™immagine a â€œSaint Bernardâ€ una volta.

**Considerazioni Finali**

- **Robustezza Complessiva**: Lâ€™enorme miglioramento rispetto ai singoli modelli (e rispetto alle precedenti valutazioni) conferma che lâ€™ensemble mitiga gli errori specifici, portando a risultati quasi perfetti anche su classi meno rappresentate.
- **Margini di Ottimizzazione**: I pochi errori residui su â€œAmerican Bulldogâ€ e â€œSaint Bernardâ€ suggeriscono possibili affinamenti, per esempio un bilanciamento ancora piÃ¹ mirato o unâ€™ulteriore revisione dei campioni borderline.
- **Suggerimento**: Unâ€™analisi qualitativa dei pochi hard examples potrebbe rivelare pattern ricorrenti (angolazioni particolari, condizioni di luce) da includere in unâ€™ulteriore fase di augmentation.

In sintesi, lâ€™ensemble su hold-out conferma un livello di performance estremamente elevato, con una distribuzione bilanciata delle metriche e solo pochissimi spunti per un ottimale perfezionamento.
"""

# --------------------------------------------------
# MATRICE DI CONFUSIONE DELLâ€™ENSEMBLE SU HOLD-OUT
# --------------------------------------------------
# Calcolo della matrice di confusione utilizzando le etichette vere e le predizioni fatte dall'ensemble.
cm = confusion_matrix(labels_hold, preds_hold)

plt.figure(figsize=(12, 10))  # Definisce le dimensioni della figura
sns.heatmap(
    cm,                        # La matrice di confusione
    annot=True,                 # Aggiunge i numeri di ciascuna cella
    fmt='d',                    # Formato numeri: interi
    cmap='Blues',               # Colori heatmap (Blues per una gradazione di blu)
    xticklabels=train_dataset.classes,  # Etichette classi asse X (predette)
    yticklabels=train_dataset.classes   # Etichette classi asse Y (reali)
)

# Aggiunta delle etichette degli assi e del titolo
plt.xlabel("Predetta")
plt.ylabel("Reale")
plt.title("Confusion Matrix Ensemble su Hold-out")

# Rotazione delle etichette sugli assi per migliorare la leggibilitÃ 
plt.xticks(rotation=45, ha='right')  # Ruota le etichette X di 45Â° per una migliore visualizzazione
plt.yticks(rotation=0)               # Etichette Y orizzontali

# Ottimizzazione del layout per evitare sovrapposizioni
plt.tight_layout()
plt.show()

"""La Matrice di confusione conferma visivamente un pattern di classificazione quasi perfetto, con valori molto elevati lungo la diagonale principale (True Positives), e assenza quasi totale di errori fuori diagonale.

- **Pattern generali osservabili nella heatmap**: Distribuzione diagonale dominante: ogni classe mostra un blocco intenso sulla diagonale, a indicare che la quasi totalitÃ  delle istanze Ã¨ stata correttamente classificata.
- **Assenza di rumore sistematico**: non ci sono segnali evidenti di confusione ricorrente tra classi simili (es. tra razze di cani o di gatti), spesso problematiche nei modelli standard.
- **Colonne e righe prevalentemente nulle**: le zone fuori diagonale (FP e FN) sono per lo piÃ¹ vuote, tranne due eccezioni lievi.

**Anomalie isolate visibili**:
- American Bulldog:
 - Il modello ha saltato una istanza reale di questa classe.
 - Probabile lieve confusione con una razza canina simile.
 - Nella heatmap si manifesta come un piccolo valore fuori diagonale in corrispondenza di una riga secondaria.

- Saint Bernard:
 - Unâ€™altra classe Ã¨ stata erroneamente classificata come â€œSaint Bernardâ€.
 - Questo compare nella heatmap come un piccolo blocco nella colonna corrispondente.
 - Il modello ha riconosciuto perfettamente tutti i veri â€œSaint Bernardâ€, ma ha â€œincluso per erroreâ€ unâ€™altra immagine.

**Considerazioni finali**
- **Altissima precisione su tutte le altre 35 classi (su 37)**: perfetto equilibrio senza nessun tipo di distorsione.
- La qualitÃ  della classificazione Ã¨ molto stabile anche per le classi meno rappresentate (es. Bengal con solo 6 esempi, o Siamese con 5), segno che lâ€™ensemble gestisce bene il rischio di overfitting.
- **Assenza di bias evidenti**: il modello non favorisce nÃ© penalizza sistematicamente classi piÃ¹ comuni o simili tra loro.


"""

# --------------------------------------------------
# ESTRAZIONE E VISUALIZZAZIONE DEGLI â€œHARD EXAMPLESâ€
# --------------------------------------------------
# Individuare indici immagini classificate in maniera errata (dove le etichette reali differiscono dalle predizioni)
mis_idx = np.where(labels_hold != preds_hold)[0]
print(f"\nNumero di errori: {len(mis_idx)}\n")

# Selezione primi 8 esempi difficili
hard_examples = mis_idx[:8]

# Creazione figura con layout 2x4 per visualizzare esempi
fig, axes = plt.subplots(2, 4, figsize=(16, 8))
axes = axes.flatten()

# Iterare sugli assi e sugli indici degli hard examples
for ax, idx in zip(axes, hard_examples):
    # Recupero immagine e label dall'holdout_set (il dataset di test)
    img, _ = holdout_set[idx]  # La variabile `_` indica che la label non Ã¨ necessaria in questa fase
    true_lbl = train_dataset.classes[labels_hold[idx]]
    pred_lbl = train_dataset.classes[preds_hold[idx]]

    # Denormalizzazione immagine per poterla visualizzare correttamente (da tensor a range [0,1])
    denorm = img.permute(1, 2, 0).numpy() * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])
    ax.imshow(denorm)

    # Imposta titolo del grafico con le etichette reali e predette, in rosso per evidenziarle
    ax.set_title(f"Vero: {true_lbl}\nPred: {pred_lbl}", color='red')
    ax.axis('off')  # Disattiva gli assi per una visualizzazione piÃ¹ pulita

# Aggiunta titolo generale figura
plt.suptitle("Hard Examples dallâ€™Hold-out", fontsize=16)
plt.tight_layout()
plt.show()

"""####Grafici di comparazione"""

# Estrazione curve di training modello originale
train_losses_orig = train_res_original["train_losses"]
val_losses_orig   = train_res_original["val_losses"]
train_accs_orig   = train_res_original["train_accs"]
val_accs_orig     = train_res_original["val_accs"]

# Estrazione delle curve per il modello combinato
train_losses_comb = train_res_combined["train_losses"]
val_losses_comb   = train_res_combined["val_losses"]
train_accs_comb   = train_res_combined["train_accs"]
val_accs_comb     = train_res_combined["val_accs"]

# Plot curve di training
plot_training_curves(train_losses_orig, val_losses_orig, train_accs_orig, val_accs_orig, 'Modello Originale')
plot_training_curves(train_losses_comb, val_losses_comb, train_accs_comb, val_accs_comb, 'Modello Combinato')

"""**Grafico 1: Andamento della Loss â€“ Modello Originale**

Nel primo grafico la curva di perdita evidenzia una **discesa netta** sia per il training sia per la validazione. In particolare:

- La **train loss** cala rapidamente da valori iniziali molto elevati fino a scendere sotto 0.05 nelle epoche finali, riflettendo un adattamento estremamente efficace ai dati di addestramento.
- La **validation loss** segue fedelmente questo trend, passando da circa 1.2 a circa 0.33, senza inversioni di tendenza significative.

Il fatto che le due curve restino affiancate con un gap minimo dimostra che il modello:

1. **Non overfitta**: la perdita sui dati non utilizzati per il training continua a migliorare insieme a quella sui dati di training.
2. **Apprende caratteristiche generalizzabili**: non si limita a memorizzare campioni, ma cattura pattern utili per ogni epoca.

---

**Grafico 2: Andamento dellâ€™Accuratezza â€“ Modello Originale**

Nel secondo grafico lâ€™accuratezza sul validation set parte intorno al **70%** alla prima epoca e cresce rapidamente, superando lâ€™**80%** giÃ  entro la seconda epoca. Da lÃ¬ in poi il modello continua a migliorare in modo costante, fino ad assestarsi attorno al **90.8%** verso lâ€™ottavaâ€“nona epoca. Questo andamento indica che:

- **Apprendimento lampo**: il modello impara in fretta le caratteristiche principali delle immagini, probabilmente grazie alla qualitÃ  del dataset e a una buona inizializzazione dei pesi.
- **Sempre piÃ¹ confidenza**: lâ€™aumento regolare senza brusche oscillazioni suggerisce che, ad ogni epoca, le correzioni ai pesi sono andate nella direzione giusta, senza introdurre instabilitÃ .
- **Punto di saturazione preciso**: raggiungere un plateau cosÃ¬ alto giÃ  a metÃ  dellâ€™addestramento mostra che pochi ulteriori aggiustamenti migliorano significativamente lâ€™accuracy.

---

**Grafico 3: Andamento della Loss â€“ Modello Combinato**

La loss del modello combinato presenta una forma un poâ€™ piÃ¹ irregolare, con la validation loss che mostra piccoli plateau ma senza rialzi marcati. In dettaglio:

- La **train loss** cala velocemente fino a valori molto bassi giÃ  dopo poche epoche, sottolineando la capacitÃ  del modello di adattarsi rapidamente ai dati estesi.
- La **validation loss**, pur discendendo in modo generale, ospita leggere fasi di stagnazione: questo Ã¨ dovuto allâ€™eterogeneitÃ  introdotta dalla generative augmentation e dal MixUp.

Tutto ciÃ² implica che, nonostante un poâ€™ di â€œrumoreâ€:

1. Il modello riesce comunque a **migliorare la generalizzazione** grazie alla varietÃ  degli esempi.
2. Le tecniche di data augmentation hanno ampliato lo spazio delle caratteristiche apprese, offrendo una robustezza maggiore sul lungo periodo.

---

**Grafico 4: Andamento dellâ€™Accuratezza â€“ Modello Combinato**

Per il modello combinato (dataset originale + immagini sintetiche + MixUp), lâ€™accuracy di validazione inizia giÃ  molto alta, intorno al **96.6%**, e arriva fino al **97.8%** nei primissimi cicli. La curva appare piÃ¹ â€œdolceâ€ rispetto al modello originale:

- **Ottimo punto di partenza**: i dati sintetici aiutano il modello a partire da una base di conoscenza piÃ¹ ampia, riducendo drasticamente la necessitÃ  di lunghe fasi di apprendimento.
- **Incrementi piÃ¹ contenuti**: i guadagni in accuracy sono piÃ¹ piccoli, perchÃ© il modello Ã¨ giÃ  vicino alla saturazione. Questo Ã¨ normale quando si parte da valori elevati.
- **Assenza di crolli**: non ci sono cadute improvvise, a riprova che la sintesi dei dati e MixUp non introducono rumore tale da destabilizzare lâ€™apprendimento.

---

**Conclusione Complessiva**

Nel complesso, i due grafici evidenziano due strategie complementari:

- Il **modello originale** apprende in modo estremamente rapido e lineare, raggiungendo unâ€™alta accuratezza in poche epoche senza segni di overfitting.
- Il **modello combinato** beneficia fin da subito dei dati sintetici e delle tecniche di augmentation: parte giÃ  con performance elevate e consolida la sua generalizzazione, sebbene con curve meno â€œlisceâ€.

Lâ€™accoppiata di questi due approcci, sfruttata in un **ensemble**, permette di fondere la rapiditÃ  di convergenza del primo con la resilienza del secondo, ottenendo cosÃ¬ previsioni estremamente accurate e stabili su un ampio spettro di classi.
"""

# Valutazione qualitativa delle metriche per ogni modello
y_true_orig, y_pred_orig = evaluate_single_model(model_original, test_loader, device)
y_true_comb, y_pred_comb = evaluate_single_model(model_combined, test_loader, device)

# Plot metriche di classificazione per classe
plot_classification_metrics_by_model(y_true_orig, y_pred_orig, train_dataset.classes, 'Modello Originale')
plot_classification_metrics_by_model(y_true_comb, y_pred_comb, train_dataset.classes, 'Modello Combinato')

# Plot predizioni corrette/errate
plot_predictions_by_model(model_original, test_loader, train_dataset.classes, device, 'Modello Originale')
plot_predictions_by_model(model_combined, test_loader, train_dataset.classes, device, 'Modello Combinato')

"""1. **Immagini con Predizioni Corrette**

- **Boxer**: la tipica conformazione muscolare e la testa squadrata sono state riconosciute con precisione, nonostante la posa leggermente inclinata.  
- **Scottish Terrier**: le caratteristiche del muso e il pelo folto sono state catturate in modo affidabile.  
- **Shiba Inu**: la mimica del viso e la coda arricciata sono state identificate correttamente, confermando la robustezza del modello sui cani di taglia media.  
- **American Pit Bull Terrier**: le caratteristiche anatomiche, pur variando leggermente tra campioni, non hanno tratto in inganno il classificatore.  
- **Staffordshire Bull Terrier**: il modello distingue bene questa razza dal Pit Bull nonostante le affinitÃ  visive.  
- **Miniature Pinscher**: la corporatura esile e le orecchie appuntite sono state interpretate senza errori.

   In tutti questi esempi il modello ha saputo estrarre caratteristiche salienti (forma del muso, tipo di pelo, proporzioni) e resistere a variazioni di illuminazione o angolazioni.

2. **Immagini con Predizioni Errate**

- Un **Bengal** Ã¨ stato classificato come **Abyssinian**, probabilmente per la similitudine nei pattern del mantello maculato.  
- Un primo **Abyssinian** Ã¨ stato scambiato per **Shiba Inu**, segnalando una sovrapposizione nel riconoscimento di texture e tonalitÃ  che puÃ² confondere il modello tra felini e canidi.  
- Un secondo **Abyssinian** Ã¨ stato etichettato come **Persian**, indicando difficoltÃ  nel distinguere variazioni nella lunghezza e nella densitÃ  del pelo fra razze feline.

  Questi errori mostrano come le classi feline, in particolare, possano richiedere unâ€™attenzione maggiore ai dettagli del mantello e alla forma del muso.

3. **Considerazioni Finali**

- **Robustezza**: la stragrande maggioranza delle immagini viene classificata correttamente, soprattutto per razze con caratteristiche ben definite.  
- **Aree di miglioramento**: per ridurre la confusione tra razze feline simili (e talvolta tra felini e canidi) potrebbe essere utile:
     - Data augmentation mirata su texture e contorni del pelo.
     - Ponderazione o sample reweighting per enfatizzare le classi con confusione residua.
- **Visualizzazione**: per apprezzare meglio i risultati, si consiglia di denormalizzare le immagini prima del plot, in modo da esaminare piÃ¹ chiaramente colori e dettagli.

In sintesi, il modello dimostra ottima capacitÃ  discriminativa su gran parte delle classi, mentre alcuni felini ancora sfuggono alle caratteristiche distintive e rappresentano un naturale next step per ulteriori affinamenti.

"""

# Plot esempi di augmentations
plot_augmented_samples(train_loader, train_dataset.classes, 'Originale')
plot_augmented_samples(combined_train_loader, train_dataset.classes, 'Combinato')

"""##### **Visualizzazione dei Dati Augmentati**

I tensori mostrati coprono un range di valori che va approssimativamente da **â€“2.12** a **+2.20**, in linea con la normalizzazione \((x - \text{mean})/\text{std}\) applicata. Alcuni pattern evidenziano chiaramente le tecniche di augmentation usate:

- **Patch uniformi (CutMix)**: aree con valori stabili (es. intorno a â€“1.56 o +1.78) indicano regioni sovrapposte da altre immagini.  
- **Variazioni localizzate (RandomErasing)**: blocchi in cui i valori cambiano bruscamente, tipici di porzioni â€œcancellateâ€ o mascherate.  
- **Shift cromatici (ColorJitter)** e **flip orizzontale**: differenze asimmetriche tra canali R, G e B confermano alterazioni di luminositÃ , contrasto e orientamento.

Il **tensor delle etichette** dimostra che, nonostante le trasformazioni, ogni immagine mantiene il label corretto. La ripetizione di alcune classi (es. 23, 33, 35) riflette il campionamento equilibrato tra dati reali e sintetici nel loader.

##### **Vantaggi delle Augmentation**

- **CutMix** spinge il modello a isolare dettagli discriminativi anche in presenza di parti miste.  
- **RandomErasing** migliora la resilienza contro occlusioni parziali.  
- **ColorJitter** e **flip** riducono la dipendenza da condizioni di luce o orientamento specifici.  

##### **Conclusione**

Queste tecniche rendono il dataset di training molto piÃ¹ vario e aiutano il modello a generalizzare meglio su nuove immagini. Anche se i valori normalizzati non sono immediatamente interpretabili visivamente, una denormalizzazione prima di mostrare le immagini permetterebbe di apprezzare direttamente lâ€™effetto delle trasformazioni.
"""

# Confronto finale modelli
model_names = ['Originale', 'Combinato', 'Ensemble']
accuracies  = [res_orig['accuracy'], res_comb['accuracy'], res_ens['accuracy']]
f1_scores   = [res_orig['f1_macro'], res_comb['f1_macro'], res_ens['f1_macro']]
plot_model_comparison(model_names, accuracies, f1_scores)

"""Nel grafico dedicato al confronto delle performance, emergono chiaramente differenze significative tra i tre approcci:

- **Modello Originale**  
  Con unâ€™**Accuracy** di **0.9717** e un **F1 Macro** di **0.9717**, il modello di base mostra giÃ  unâ€™eccellente capacitÃ  di classificazione. Tuttavia, il fatto che i due valori siano identici suggerisce che alcune classi meno rappresentate hanno performance leggermente inferiori, compensate dal buon comportamento sulle classi piÃ¹ frequenti.

- **Modello Combinato**  
  Integrando dati reali e sintetici, lâ€™**Accuracy** raggiunge **0.9960** e il **F1 Macro** addirittura **0.9965**. Questo salto indica non solo una drastica riduzione degli errori complessivi, ma anche una maggiore uniformitÃ  delle prestazioni tra le classi, grazie allâ€™arricchimento del dataset e alle tecniche di augmentation.

- **Ensemble**  
  Lâ€™approccio ensemble mantiene unâ€™**Accuracy** di **0.9960** e un **F1 Macro** di **0.9963**, quasi a livello del modello combinato. CiÃ² conferma che la fusione delle predizioni rafforza la stabilitÃ  delle decisioni, attenuando eventuali debolezze residue di ciascun modello singolo.

**Osservazioni Finali**  
1. Il modello combinato dimostra il maggior guadagno in termini di precisione e uniformitÃ , evidenziando lâ€™efficacia dei dati sintetici e delle augmentation.  
2. Lâ€™ensemble sfrutta al meglio i punti di forza di entrambi, offrendo performance leggermente piÃ¹ solide in fase di validazione complessiva.  
3. In scenari reali, un approccio ensemble fornisce la miglior garanzia di affidabilitÃ  e generalizzazione, distribuendo gli errori in modo piÃ¹ omogeneo su tutte le classi.

##Conclusioni e Analisi dei Risultati

Il progetto si proponeva di potenziare il riconoscimento di oggetti e comportamenti critici nelle immagini delle centrali elettriche, superando i limiti di dataset troppo piccoli e poco rappresentativi. Grazie alla pipeline generativa â€“ che unisce BLIP per il captioning, Flan-T5 per le varianti testuali e Stable Diffusion per produrre nuove immagini â€“ siamo riusciti non solo ad aumentare la quantitÃ  di dati, ma soprattutto la loro varietÃ , coprendo meglio scenari reali complessi.

1. **Apprendimento â€œBaseâ€ (dataset originale)**

  Allâ€™inizio, il modello addestrato unicamente sul dataset Oxford-IIIT Pet mostrava un progresso rapido, ma presto finiva per â€œingolfarsiâ€ sulle stesse informazioni: giÃ  dopo cinque-sei epoche lâ€™accuracy di validazione si stabilizzava intorno al 90 %, mentre la loss diminuiva di molto piÃ¹ lentamente. Era il chiaro segnale che, una volta acquisite le caratteristiche piÃ¹ comuni delle immagini delle centrali, mancava al modello la variabilitÃ  necessaria per spingersi oltre, lasciando in circolazione un numero non trascurabile di falsi positivi e falsi negativi.

2. **Apprendimento â€œEstesoâ€ (immagini sintetiche)**

  Non appena abbiamo introdotto le immagini generate da BLIP, Flan-T5 e Stable Diffusion, lâ€™intera dinamica di training Ã¨ cambiata:
  - Spinta fin dalle prime epoche: giÃ  alla 2Âªâ€“3Âª lâ€™accuracy di validazione superava il 95 %, contro il 90 % del modello base.
  - Apprendimento continuativo: la loss scendeva in modo costante fino allâ€™early stopping alla 12Âª epoca, prolungando la fase in cui il modello â€œscoprivaâ€ nuova informazione grazie ai pattern aggiuntivi.
  - Guadagno netto in validazione e test: +6 % di val acc (da 90 % â†’ 96,5 %) e salto di test acc dal 97 % al 99 %.

  - UniformitÃ  delle metriche: precision, recall e F1-macro, che sul solo dataset originale si attestavano intorno a 0.97, sono saliti quasi a 0.99 su tutte le classiâ€”non solo sulle piÃ¹ popolose, ma anche su quelle piÃ¹ rare.

  Questo miglioramento non Ã¨ stato un mero aggiustamento numerico, ma il risultato di una maggiore diversitÃ  di esempi: ogni batch portava con sÃ© contesti e angolazioni nuovi, estendendo la generalizzazione del modello anche ai casi meno rappresentati.

3. **Apprendimento â€œEnsembleâ€ (modello base + esteso)**

  Per chiudere il cerchio abbiamo combinato i due modelli in un ensemble:
  - Robustezza ai margini: lâ€™accuracy rimane stabile attorno al 99 % sia in validazione sia sul test set, riducendo la varianza quando lâ€™input Ã¨ borderline.
  - Compensazione degli errori: se un modello esitava su unâ€™immagine ambigua, lâ€™altro â€œcoprivaâ€ lâ€™errore, azzerando quasi del tutto i falsi positivi/negativi residui.
  - Hard examples: i pochi errori rimasti sullâ€™hold-out riguardano scene visivamente molto similiâ€”forme e colori affini di componenti meccanicheâ€”indicando che la prossima iterazione di captioning potrebbe concentrarsi proprio su questi scenari di massima confusione.

In sintesi, dalla fase â€œBaseâ€ con saturazione precoce, allâ€™apprendimento â€œEstesoâ€ potenziato da dati generativi, fino alla stabilitÃ  dellâ€™â€œEnsembleâ€, ogni passaggio ha dimostrato come la generative augmentation non solo aumenti i numeri di training, ma trasformi profondamente la curva di apprendimento, estendendo la fase di scoperta e migliorando uniformemente la capacitÃ  di riconoscere comportamenti critici nelle immagini delle centrali. Per CyberEye Solutions, significa poter contare su un sistema di sorveglianza delle infrastrutture critiche estremamente preciso, rapido nel convergere e resistente alle situazioni piÃ¹ complesse.

---

Per concludere, la strategia di generative augmentation ha dimostrato di essere molto piÃ¹ di una semplice â€œquantitÃ  in piÃ¹â€: ha arricchito la qualitÃ  e la diversitÃ  del dataset, portando a un modello di riconoscimento immagini che accelera il training (riducendo le epoche necessarie), alza il livello di accuratezza (da ~90 % a ~99 %) e migliora in modo uniforme precision, recall e F1 su tutte le classi. Per CyberEye Solutions, questo significa una sorveglianza delle centrali elettriche piÃ¹ rapida, affidabile e capace di intercettare tempestivamente comportamenti o oggetti potenzialmente pericolosi, ridefinendo gli standard di sicurezza delle infrastrutture critiche.
"""